{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip cells.zip\n",
        "#Here cells.zip is the zip file containing all the images of the cells."
      ],
      "metadata": {
        "id": "wQ3G_RZA36Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2lfeMGwVtE8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading Labels from CSV file\n",
        "df = pd.read_csv(\"label.csv\",header = 0)\n",
        "data = []\n",
        "# Loading the images and preprocessing them - converting to array and reshaping\n",
        "for i in range(1,df.shape[0]+1):\n",
        "    img_path = 'cells/{}.png'.format(i)\n",
        "    img = keras.preprocessing.image.load_img(img_path, color_mode= 'grayscale', target_size=(100, 100))\n",
        "    img_array = keras.preprocessing.image.img_to_array(img)\n",
        "    data.append(img_array)\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(df['label'])\n",
        "labels = labels - 1 # Labels stay in range 0 to 5\n",
        "\n",
        "#Randomizing the order of the data\n",
        "shuffle = np.random.permutation(len(data))\n",
        "data = data[shuffle]\n",
        "labels = labels[shuffle]\n",
        "\n",
        "# Spliting the data for validation and training.\n",
        "validation_split = 0.1\n",
        "split_index = int((1 - validation_split) * len(data))\n",
        "train_data = data[:split_index]\n",
        "train_labels = labels[:split_index]\n",
        "val_data = data[split_index:]\n",
        "val_labels = labels[split_index:]\n"
      ],
      "metadata": {
        "id": "dFaDFCn1nNzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Structure of the CNN model.\n",
        "model = keras.Sequential()\n",
        "#The first convolutional layer - has 32 filters\n",
        "model.add(layers.Conv2D(32, kernel_size= (3,3), activation = 'relu', input_shape = (100,100,1)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "#The second convolutional layer - has 64 filters\n",
        "model.add(layers.Conv2D(64, kernel_size = (3,3), activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64,activation = 'relu'))\n",
        "model.add(layers.Dense(6,activation = 'softmax'))\n",
        "\n",
        "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer =\"adam\", metrics = ['accuracy'])\n",
        "\n",
        "trained_model = model.fit(train_data,train_labels,epochs = 10, validation_data = (val_data,val_labels))"
      ],
      "metadata": {
        "id": "1xkZVFZViFkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model is evaluated and Loss and Accuracy is calculated\n",
        "evaluation = model.evaluate(val_data, val_labels)\n",
        "print(\"Evaluation Loss:\", evaluation[0])\n",
        "print(\"Evaluation Accuracy:\", evaluation[1])\n",
        "\n",
        "# Precision, Recall, and F1 Score are calculated\n",
        "predictions = model.predict(val_data)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "precision = precision_score(val_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(val_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(val_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "#Plot of Training Loss and Validation Loss after each Epoch\n",
        "plt.plot(trained_model.history['loss'], label='Training Loss')\n",
        "plt.plot(trained_model.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#Plot of Training Accuracy and Validation Accuracy after each Epoch\n",
        "plt.plot(trained_model.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(trained_model.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mysw0DYSgDM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A CNN with two Convolution layers has been implemented. Metrics such as Accuracy, Loss, Precision, Recall and F1 Score have been calculated. Training and Validation loss and accuracy have been plotted\n",
        "\n",
        "The no of Epochs has been set at 10. However after 6th/7th epoch, the model starts to overfit, as the validation loss increases and validation accuracy decreases slightly.\n",
        "\n",
        "The accuracy of the model on training data increases after each epoch, however the accuracy on validation data increases until a certain no of epochs and then decreases. This is because the model starts to overfit - it can be said that the model starts to memorize the training data.\n",
        "\n",
      ],
      "metadata": {
        "id": "8g_zBqfLijPI"
      }
    }
  ]
}